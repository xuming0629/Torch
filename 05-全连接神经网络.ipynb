{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例1 MNIST-手写数字识别系统\n",
    "+ 算法思路\n",
    "    > 准备训练参数\n",
    "\n",
    "    > 准备数据集(训练集和测试集)\n",
    "\n",
    "    > 建立神经网络模型\n",
    "\n",
    "    > 选择优化算法, 计算 loss\n",
    "    \n",
    "    > 训练集上训练模型\n",
    "    \n",
    "    > 测试集上检验正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 \n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "\n",
    "# train_dataset\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=True,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "# test_dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                         train=False,\n",
    "                                         transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'>\n",
      "<class 'torchvision.datasets.mnist.MNIST'>\n",
      "训练的图片数量: 60000\n",
      "测试的图片数量: 10000\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "\n",
    "print('训练的图片数量:', len(train_dataset))\n",
    "print('测试的图片数量:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: ./data/\n",
       "    Transforms (if any): ToTensor()\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader (input pipeline)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,   # 导出图片数据集\n",
    "                                           batch_size=batch_size,   # 训练选取多少张图片\n",
    "                                           shuffle=True)            # 采取多少个子进程\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练集中的前五张图片.\n",
    "\n",
    "indicator = 0\n",
    "for image, label in train_loader:\n",
    "    if indicator >=3:\n",
    "        break\n",
    "    print(label[0])\n",
    "    img = transforms.ToPILImage()(image[0])\n",
    "    img.show()\n",
    "    # 显示图片\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    indicator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络模型\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        out2 = self.relu(out1)\n",
    "        out = self.fc2(out2)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "\n",
    "mlp_model = Net(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "ｃｒiterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义迭代参数的算法\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/600], Loss: 2.3257\n",
      "Epoch [1/100], Step [200/600], Loss: 2.2951\n",
      "Epoch [1/100], Step [300/600], Loss: 2.2938\n",
      "Epoch [1/100], Step [400/600], Loss: 2.3216\n",
      "Epoch [1/100], Step [500/600], Loss: 2.3267\n",
      "Epoch [1/100], Step [600/600], Loss: 2.3035\n",
      "Epoch [2/100], Step [100/600], Loss: 2.2965\n",
      "Epoch [2/100], Step [200/600], Loss: 2.3281\n",
      "Epoch [2/100], Step [300/600], Loss: 2.3049\n",
      "Epoch [2/100], Step [400/600], Loss: 2.3174\n",
      "Epoch [2/100], Step [500/600], Loss: 2.3050\n",
      "Epoch [2/100], Step [600/600], Loss: 2.3296\n",
      "Epoch [3/100], Step [100/600], Loss: 2.3139\n",
      "Epoch [3/100], Step [200/600], Loss: 2.3176\n",
      "Epoch [3/100], Step [300/600], Loss: 2.3153\n",
      "Epoch [3/100], Step [400/600], Loss: 2.3077\n",
      "Epoch [3/100], Step [500/600], Loss: 2.3126\n",
      "Epoch [3/100], Step [600/600], Loss: 2.3232\n",
      "Epoch [4/100], Step [100/600], Loss: 2.3219\n",
      "Epoch [4/100], Step [200/600], Loss: 2.3265\n",
      "Epoch [4/100], Step [300/600], Loss: 2.3223\n",
      "Epoch [4/100], Step [400/600], Loss: 2.3044\n",
      "Epoch [4/100], Step [500/600], Loss: 2.2988\n",
      "Epoch [4/100], Step [600/600], Loss: 2.3003\n",
      "Epoch [5/100], Step [100/600], Loss: 2.3175\n",
      "Epoch [5/100], Step [200/600], Loss: 2.3195\n",
      "Epoch [5/100], Step [300/600], Loss: 2.3259\n",
      "Epoch [5/100], Step [400/600], Loss: 2.3160\n",
      "Epoch [5/100], Step [500/600], Loss: 2.3200\n",
      "Epoch [5/100], Step [600/600], Loss: 2.3162\n",
      "Epoch [6/100], Step [100/600], Loss: 2.2934\n",
      "Epoch [6/100], Step [200/600], Loss: 2.2947\n",
      "Epoch [6/100], Step [300/600], Loss: 2.3265\n",
      "Epoch [6/100], Step [400/600], Loss: 2.3102\n",
      "Epoch [6/100], Step [500/600], Loss: 2.3022\n",
      "Epoch [6/100], Step [600/600], Loss: 2.3118\n",
      "Epoch [7/100], Step [100/600], Loss: 2.3174\n",
      "Epoch [7/100], Step [200/600], Loss: 2.3174\n",
      "Epoch [7/100], Step [300/600], Loss: 2.3214\n",
      "Epoch [7/100], Step [400/600], Loss: 2.3190\n",
      "Epoch [7/100], Step [500/600], Loss: 2.3031\n",
      "Epoch [7/100], Step [600/600], Loss: 2.3002\n",
      "Epoch [8/100], Step [100/600], Loss: 2.3093\n",
      "Epoch [8/100], Step [200/600], Loss: 2.3022\n",
      "Epoch [8/100], Step [300/600], Loss: 2.2875\n",
      "Epoch [8/100], Step [400/600], Loss: 2.3072\n",
      "Epoch [8/100], Step [500/600], Loss: 2.3094\n",
      "Epoch [8/100], Step [600/600], Loss: 2.3129\n",
      "Epoch [9/100], Step [100/600], Loss: 2.3098\n",
      "Epoch [9/100], Step [200/600], Loss: 2.3157\n",
      "Epoch [9/100], Step [300/600], Loss: 2.3119\n",
      "Epoch [9/100], Step [400/600], Loss: 2.3011\n",
      "Epoch [9/100], Step [500/600], Loss: 2.3094\n",
      "Epoch [9/100], Step [600/600], Loss: 2.3036\n",
      "Epoch [10/100], Step [100/600], Loss: 2.3039\n",
      "Epoch [10/100], Step [200/600], Loss: 2.3082\n",
      "Epoch [10/100], Step [300/600], Loss: 2.3083\n",
      "Epoch [10/100], Step [400/600], Loss: 2.3207\n",
      "Epoch [10/100], Step [500/600], Loss: 2.3166\n",
      "Epoch [10/100], Step [600/600], Loss: 2.3153\n",
      "Epoch [11/100], Step [100/600], Loss: 2.2990\n",
      "Epoch [11/100], Step [200/600], Loss: 2.3102\n",
      "Epoch [11/100], Step [300/600], Loss: 2.3091\n",
      "Epoch [11/100], Step [400/600], Loss: 2.3226\n",
      "Epoch [11/100], Step [500/600], Loss: 2.3023\n",
      "Epoch [11/100], Step [600/600], Loss: 2.2992\n",
      "Epoch [12/100], Step [100/600], Loss: 2.3043\n",
      "Epoch [12/100], Step [200/600], Loss: 2.3297\n",
      "Epoch [12/100], Step [300/600], Loss: 2.3095\n",
      "Epoch [12/100], Step [400/600], Loss: 2.3162\n",
      "Epoch [12/100], Step [500/600], Loss: 2.3027\n",
      "Epoch [12/100], Step [600/600], Loss: 2.3153\n",
      "Epoch [13/100], Step [100/600], Loss: 2.3033\n",
      "Epoch [13/100], Step [200/600], Loss: 2.3155\n",
      "Epoch [13/100], Step [300/600], Loss: 2.3115\n",
      "Epoch [13/100], Step [400/600], Loss: 2.3070\n",
      "Epoch [13/100], Step [500/600], Loss: 2.3076\n",
      "Epoch [13/100], Step [600/600], Loss: 2.3259\n",
      "Epoch [14/100], Step [100/600], Loss: 2.3152\n",
      "Epoch [14/100], Step [200/600], Loss: 2.3006\n",
      "Epoch [14/100], Step [300/600], Loss: 2.3040\n",
      "Epoch [14/100], Step [400/600], Loss: 2.3237\n",
      "Epoch [14/100], Step [500/600], Loss: 2.3059\n",
      "Epoch [14/100], Step [600/600], Loss: 2.3169\n",
      "Epoch [15/100], Step [100/600], Loss: 2.3102\n",
      "Epoch [15/100], Step [200/600], Loss: 2.3220\n",
      "Epoch [15/100], Step [300/600], Loss: 2.3215\n",
      "Epoch [15/100], Step [400/600], Loss: 2.3182\n",
      "Epoch [15/100], Step [500/600], Loss: 2.3182\n",
      "Epoch [15/100], Step [600/600], Loss: 2.3264\n",
      "Epoch [16/100], Step [100/600], Loss: 2.3027\n",
      "Epoch [16/100], Step [200/600], Loss: 2.3105\n",
      "Epoch [16/100], Step [300/600], Loss: 2.3208\n",
      "Epoch [16/100], Step [400/600], Loss: 2.3030\n",
      "Epoch [16/100], Step [500/600], Loss: 2.2966\n",
      "Epoch [16/100], Step [600/600], Loss: 2.2985\n",
      "Epoch [17/100], Step [100/600], Loss: 2.3210\n",
      "Epoch [17/100], Step [200/600], Loss: 2.2891\n",
      "Epoch [17/100], Step [300/600], Loss: 2.3234\n",
      "Epoch [17/100], Step [400/600], Loss: 2.2989\n",
      "Epoch [17/100], Step [500/600], Loss: 2.3151\n",
      "Epoch [17/100], Step [600/600], Loss: 2.3004\n",
      "Epoch [18/100], Step [100/600], Loss: 2.3152\n",
      "Epoch [18/100], Step [200/600], Loss: 2.2934\n",
      "Epoch [18/100], Step [300/600], Loss: 2.3182\n",
      "Epoch [18/100], Step [400/600], Loss: 2.3118\n",
      "Epoch [18/100], Step [500/600], Loss: 2.3204\n",
      "Epoch [18/100], Step [600/600], Loss: 2.2965\n",
      "Epoch [19/100], Step [100/600], Loss: 2.3195\n",
      "Epoch [19/100], Step [200/600], Loss: 2.3128\n",
      "Epoch [19/100], Step [300/600], Loss: 2.3302\n",
      "Epoch [19/100], Step [400/600], Loss: 2.3142\n",
      "Epoch [19/100], Step [500/600], Loss: 2.2955\n",
      "Epoch [19/100], Step [600/600], Loss: 2.3121\n",
      "Epoch [20/100], Step [100/600], Loss: 2.3296\n",
      "Epoch [20/100], Step [200/600], Loss: 2.3133\n",
      "Epoch [20/100], Step [300/600], Loss: 2.3023\n",
      "Epoch [20/100], Step [400/600], Loss: 2.3043\n",
      "Epoch [20/100], Step [500/600], Loss: 2.3164\n",
      "Epoch [20/100], Step [600/600], Loss: 2.3168\n",
      "Epoch [21/100], Step [100/600], Loss: 2.3047\n",
      "Epoch [21/100], Step [200/600], Loss: 2.3109\n",
      "Epoch [21/100], Step [300/600], Loss: 2.2913\n",
      "Epoch [21/100], Step [400/600], Loss: 2.2994\n",
      "Epoch [21/100], Step [500/600], Loss: 2.3178\n",
      "Epoch [21/100], Step [600/600], Loss: 2.3024\n",
      "Epoch [22/100], Step [100/600], Loss: 2.2988\n",
      "Epoch [22/100], Step [200/600], Loss: 2.3083\n",
      "Epoch [22/100], Step [300/600], Loss: 2.3124\n",
      "Epoch [22/100], Step [400/600], Loss: 2.3490\n",
      "Epoch [22/100], Step [500/600], Loss: 2.3076\n",
      "Epoch [22/100], Step [600/600], Loss: 2.3038\n",
      "Epoch [23/100], Step [100/600], Loss: 2.3149\n",
      "Epoch [23/100], Step [200/600], Loss: 2.2966\n",
      "Epoch [23/100], Step [300/600], Loss: 2.3102\n",
      "Epoch [23/100], Step [400/600], Loss: 2.3009\n",
      "Epoch [23/100], Step [500/600], Loss: 2.3173\n",
      "Epoch [23/100], Step [600/600], Loss: 2.3209\n",
      "Epoch [24/100], Step [100/600], Loss: 2.3075\n",
      "Epoch [24/100], Step [200/600], Loss: 2.3015\n",
      "Epoch [24/100], Step [300/600], Loss: 2.3154\n",
      "Epoch [24/100], Step [400/600], Loss: 2.3089\n",
      "Epoch [24/100], Step [500/600], Loss: 2.3122\n",
      "Epoch [24/100], Step [600/600], Loss: 2.3046\n",
      "Epoch [25/100], Step [100/600], Loss: 2.3033\n",
      "Epoch [25/100], Step [200/600], Loss: 2.3105\n",
      "Epoch [25/100], Step [300/600], Loss: 2.3040\n",
      "Epoch [25/100], Step [400/600], Loss: 2.3101\n",
      "Epoch [25/100], Step [500/600], Loss: 2.3155\n",
      "Epoch [25/100], Step [600/600], Loss: 2.3059\n",
      "Epoch [26/100], Step [100/600], Loss: 2.3133\n",
      "Epoch [26/100], Step [200/600], Loss: 2.3201\n",
      "Epoch [26/100], Step [300/600], Loss: 2.3005\n",
      "Epoch [26/100], Step [400/600], Loss: 2.3071\n",
      "Epoch [26/100], Step [500/600], Loss: 2.2969\n",
      "Epoch [26/100], Step [600/600], Loss: 2.3092\n",
      "Epoch [27/100], Step [100/600], Loss: 2.3084\n",
      "Epoch [27/100], Step [200/600], Loss: 2.3028\n",
      "Epoch [27/100], Step [300/600], Loss: 2.3171\n",
      "Epoch [27/100], Step [400/600], Loss: 2.3112\n",
      "Epoch [27/100], Step [500/600], Loss: 2.2887\n",
      "Epoch [27/100], Step [600/600], Loss: 2.3102\n",
      "Epoch [28/100], Step [100/600], Loss: 2.3087\n",
      "Epoch [28/100], Step [200/600], Loss: 2.3235\n",
      "Epoch [28/100], Step [300/600], Loss: 2.3144\n",
      "Epoch [28/100], Step [400/600], Loss: 2.3150\n",
      "Epoch [28/100], Step [500/600], Loss: 2.2962\n",
      "Epoch [28/100], Step [600/600], Loss: 2.3101\n",
      "Epoch [29/100], Step [100/600], Loss: 2.3073\n",
      "Epoch [29/100], Step [200/600], Loss: 2.3124\n",
      "Epoch [29/100], Step [300/600], Loss: 2.3023\n",
      "Epoch [29/100], Step [400/600], Loss: 2.3234\n",
      "Epoch [29/100], Step [500/600], Loss: 2.2976\n",
      "Epoch [29/100], Step [600/600], Loss: 2.2961\n",
      "Epoch [30/100], Step [100/600], Loss: 2.3197\n",
      "Epoch [30/100], Step [200/600], Loss: 2.3092\n",
      "Epoch [30/100], Step [300/600], Loss: 2.3176\n",
      "Epoch [30/100], Step [400/600], Loss: 2.2996\n",
      "Epoch [30/100], Step [500/600], Loss: 2.3060\n",
      "Epoch [30/100], Step [600/600], Loss: 2.3026\n",
      "Epoch [31/100], Step [100/600], Loss: 2.3157\n",
      "Epoch [31/100], Step [200/600], Loss: 2.3040\n",
      "Epoch [31/100], Step [300/600], Loss: 2.2936\n",
      "Epoch [31/100], Step [400/600], Loss: 2.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [500/600], Loss: 2.3183\n",
      "Epoch [31/100], Step [600/600], Loss: 2.3224\n",
      "Epoch [32/100], Step [100/600], Loss: 2.3191\n",
      "Epoch [32/100], Step [200/600], Loss: 2.3020\n",
      "Epoch [32/100], Step [300/600], Loss: 2.2952\n",
      "Epoch [32/100], Step [400/600], Loss: 2.3277\n",
      "Epoch [32/100], Step [500/600], Loss: 2.3117\n",
      "Epoch [32/100], Step [600/600], Loss: 2.3123\n",
      "Epoch [33/100], Step [100/600], Loss: 2.3032\n",
      "Epoch [33/100], Step [200/600], Loss: 2.3112\n",
      "Epoch [33/100], Step [300/600], Loss: 2.3280\n",
      "Epoch [33/100], Step [400/600], Loss: 2.3092\n",
      "Epoch [33/100], Step [500/600], Loss: 2.3166\n",
      "Epoch [33/100], Step [600/600], Loss: 2.3092\n",
      "Epoch [34/100], Step [100/600], Loss: 2.3040\n",
      "Epoch [34/100], Step [200/600], Loss: 2.2994\n",
      "Epoch [34/100], Step [300/600], Loss: 2.3201\n",
      "Epoch [34/100], Step [400/600], Loss: 2.3129\n",
      "Epoch [34/100], Step [500/600], Loss: 2.3184\n",
      "Epoch [34/100], Step [600/600], Loss: 2.3024\n",
      "Epoch [35/100], Step [100/600], Loss: 2.3015\n",
      "Epoch [35/100], Step [200/600], Loss: 2.3219\n",
      "Epoch [35/100], Step [300/600], Loss: 2.3147\n",
      "Epoch [35/100], Step [400/600], Loss: 2.3155\n",
      "Epoch [35/100], Step [500/600], Loss: 2.3073\n",
      "Epoch [35/100], Step [600/600], Loss: 2.2977\n",
      "Epoch [36/100], Step [100/600], Loss: 2.3335\n",
      "Epoch [36/100], Step [200/600], Loss: 2.3196\n",
      "Epoch [36/100], Step [300/600], Loss: 2.3195\n",
      "Epoch [36/100], Step [400/600], Loss: 2.3190\n",
      "Epoch [36/100], Step [500/600], Loss: 2.3198\n",
      "Epoch [36/100], Step [600/600], Loss: 2.3005\n",
      "Epoch [37/100], Step [100/600], Loss: 2.2951\n",
      "Epoch [37/100], Step [200/600], Loss: 2.2960\n",
      "Epoch [37/100], Step [300/600], Loss: 2.3142\n",
      "Epoch [37/100], Step [400/600], Loss: 2.3031\n",
      "Epoch [37/100], Step [500/600], Loss: 2.3142\n",
      "Epoch [37/100], Step [600/600], Loss: 2.3228\n",
      "Epoch [38/100], Step [100/600], Loss: 2.2932\n",
      "Epoch [38/100], Step [200/600], Loss: 2.3071\n",
      "Epoch [38/100], Step [300/600], Loss: 2.3222\n",
      "Epoch [38/100], Step [400/600], Loss: 2.3079\n",
      "Epoch [38/100], Step [500/600], Loss: 2.3143\n",
      "Epoch [38/100], Step [600/600], Loss: 2.3232\n",
      "Epoch [39/100], Step [100/600], Loss: 2.3192\n",
      "Epoch [39/100], Step [200/600], Loss: 2.3345\n",
      "Epoch [39/100], Step [300/600], Loss: 2.3216\n",
      "Epoch [39/100], Step [400/600], Loss: 2.3336\n",
      "Epoch [39/100], Step [500/600], Loss: 2.3182\n",
      "Epoch [39/100], Step [600/600], Loss: 2.3230\n",
      "Epoch [40/100], Step [100/600], Loss: 2.3245\n",
      "Epoch [40/100], Step [200/600], Loss: 2.3201\n",
      "Epoch [40/100], Step [300/600], Loss: 2.3226\n",
      "Epoch [40/100], Step [400/600], Loss: 2.3184\n",
      "Epoch [40/100], Step [500/600], Loss: 2.3073\n",
      "Epoch [40/100], Step [600/600], Loss: 2.3030\n",
      "Epoch [41/100], Step [100/600], Loss: 2.3201\n",
      "Epoch [41/100], Step [200/600], Loss: 2.3274\n",
      "Epoch [41/100], Step [300/600], Loss: 2.3088\n",
      "Epoch [41/100], Step [400/600], Loss: 2.3190\n",
      "Epoch [41/100], Step [500/600], Loss: 2.3056\n",
      "Epoch [41/100], Step [600/600], Loss: 2.3246\n",
      "Epoch [42/100], Step [100/600], Loss: 2.3156\n",
      "Epoch [42/100], Step [200/600], Loss: 2.2970\n",
      "Epoch [42/100], Step [300/600], Loss: 2.3225\n",
      "Epoch [42/100], Step [400/600], Loss: 2.3203\n",
      "Epoch [42/100], Step [500/600], Loss: 2.3046\n",
      "Epoch [42/100], Step [600/600], Loss: 2.3112\n",
      "Epoch [43/100], Step [100/600], Loss: 2.3184\n",
      "Epoch [43/100], Step [200/600], Loss: 2.3134\n",
      "Epoch [43/100], Step [300/600], Loss: 2.3067\n",
      "Epoch [43/100], Step [400/600], Loss: 2.3289\n",
      "Epoch [43/100], Step [500/600], Loss: 2.3064\n",
      "Epoch [43/100], Step [600/600], Loss: 2.3030\n",
      "Epoch [44/100], Step [100/600], Loss: 2.3095\n",
      "Epoch [44/100], Step [200/600], Loss: 2.3121\n",
      "Epoch [44/100], Step [300/600], Loss: 2.2935\n",
      "Epoch [44/100], Step [400/600], Loss: 2.3058\n",
      "Epoch [44/100], Step [500/600], Loss: 2.3202\n",
      "Epoch [44/100], Step [600/600], Loss: 2.3188\n",
      "Epoch [45/100], Step [100/600], Loss: 2.3189\n",
      "Epoch [45/100], Step [200/600], Loss: 2.3228\n",
      "Epoch [45/100], Step [300/600], Loss: 2.2938\n",
      "Epoch [45/100], Step [400/600], Loss: 2.3050\n",
      "Epoch [45/100], Step [500/600], Loss: 2.3109\n",
      "Epoch [45/100], Step [600/600], Loss: 2.3120\n",
      "Epoch [46/100], Step [100/600], Loss: 2.3188\n",
      "Epoch [46/100], Step [200/600], Loss: 2.3048\n",
      "Epoch [46/100], Step [300/600], Loss: 2.3217\n",
      "Epoch [46/100], Step [400/600], Loss: 2.2834\n",
      "Epoch [46/100], Step [500/600], Loss: 2.3042\n",
      "Epoch [46/100], Step [600/600], Loss: 2.3101\n",
      "Epoch [47/100], Step [100/600], Loss: 2.2852\n",
      "Epoch [47/100], Step [200/600], Loss: 2.3037\n",
      "Epoch [47/100], Step [300/600], Loss: 2.3085\n",
      "Epoch [47/100], Step [400/600], Loss: 2.3195\n",
      "Epoch [47/100], Step [500/600], Loss: 2.3143\n",
      "Epoch [47/100], Step [600/600], Loss: 2.3260\n",
      "Epoch [48/100], Step [100/600], Loss: 2.3044\n",
      "Epoch [48/100], Step [200/600], Loss: 2.3076\n",
      "Epoch [48/100], Step [300/600], Loss: 2.3285\n",
      "Epoch [48/100], Step [400/600], Loss: 2.3328\n",
      "Epoch [48/100], Step [500/600], Loss: 2.3087\n",
      "Epoch [48/100], Step [600/600], Loss: 2.3140\n",
      "Epoch [49/100], Step [100/600], Loss: 2.3103\n",
      "Epoch [49/100], Step [200/600], Loss: 2.2859\n",
      "Epoch [49/100], Step [300/600], Loss: 2.3247\n",
      "Epoch [49/100], Step [400/600], Loss: 2.3165\n",
      "Epoch [49/100], Step [500/600], Loss: 2.3130\n",
      "Epoch [49/100], Step [600/600], Loss: 2.3067\n",
      "Epoch [50/100], Step [100/600], Loss: 2.2980\n",
      "Epoch [50/100], Step [200/600], Loss: 2.3214\n",
      "Epoch [50/100], Step [300/600], Loss: 2.3111\n",
      "Epoch [50/100], Step [400/600], Loss: 2.3117\n",
      "Epoch [50/100], Step [500/600], Loss: 2.3147\n",
      "Epoch [50/100], Step [600/600], Loss: 2.3233\n",
      "Epoch [51/100], Step [100/600], Loss: 2.3033\n",
      "Epoch [51/100], Step [200/600], Loss: 2.3251\n",
      "Epoch [51/100], Step [300/600], Loss: 2.3365\n",
      "Epoch [51/100], Step [400/600], Loss: 2.3147\n",
      "Epoch [51/100], Step [500/600], Loss: 2.3037\n",
      "Epoch [51/100], Step [600/600], Loss: 2.2809\n",
      "Epoch [52/100], Step [100/600], Loss: 2.3195\n",
      "Epoch [52/100], Step [200/600], Loss: 2.3182\n",
      "Epoch [52/100], Step [300/600], Loss: 2.3082\n",
      "Epoch [52/100], Step [400/600], Loss: 2.3187\n",
      "Epoch [52/100], Step [500/600], Loss: 2.3081\n",
      "Epoch [52/100], Step [600/600], Loss: 2.3052\n",
      "Epoch [53/100], Step [100/600], Loss: 2.3069\n",
      "Epoch [53/100], Step [200/600], Loss: 2.2998\n",
      "Epoch [53/100], Step [300/600], Loss: 2.3197\n",
      "Epoch [53/100], Step [400/600], Loss: 2.3055\n",
      "Epoch [53/100], Step [500/600], Loss: 2.3255\n",
      "Epoch [53/100], Step [600/600], Loss: 2.3269\n",
      "Epoch [54/100], Step [100/600], Loss: 2.3338\n",
      "Epoch [54/100], Step [200/600], Loss: 2.3047\n",
      "Epoch [54/100], Step [300/600], Loss: 2.3149\n",
      "Epoch [54/100], Step [400/600], Loss: 2.3145\n",
      "Epoch [54/100], Step [500/600], Loss: 2.3144\n",
      "Epoch [54/100], Step [600/600], Loss: 2.3039\n",
      "Epoch [55/100], Step [100/600], Loss: 2.3185\n",
      "Epoch [55/100], Step [200/600], Loss: 2.3129\n",
      "Epoch [55/100], Step [300/600], Loss: 2.2875\n",
      "Epoch [55/100], Step [400/600], Loss: 2.3187\n",
      "Epoch [55/100], Step [500/600], Loss: 2.3142\n",
      "Epoch [55/100], Step [600/600], Loss: 2.3091\n",
      "Epoch [56/100], Step [100/600], Loss: 2.3100\n",
      "Epoch [56/100], Step [200/600], Loss: 2.3019\n",
      "Epoch [56/100], Step [300/600], Loss: 2.3126\n",
      "Epoch [56/100], Step [400/600], Loss: 2.3056\n",
      "Epoch [56/100], Step [500/600], Loss: 2.3150\n",
      "Epoch [56/100], Step [600/600], Loss: 2.3169\n",
      "Epoch [57/100], Step [100/600], Loss: 2.3197\n",
      "Epoch [57/100], Step [200/600], Loss: 2.3131\n",
      "Epoch [57/100], Step [300/600], Loss: 2.3303\n",
      "Epoch [57/100], Step [400/600], Loss: 2.3102\n",
      "Epoch [57/100], Step [500/600], Loss: 2.3182\n",
      "Epoch [57/100], Step [600/600], Loss: 2.3210\n",
      "Epoch [58/100], Step [100/600], Loss: 2.3003\n",
      "Epoch [58/100], Step [200/600], Loss: 2.3247\n",
      "Epoch [58/100], Step [300/600], Loss: 2.3097\n",
      "Epoch [58/100], Step [400/600], Loss: 2.2918\n",
      "Epoch [58/100], Step [500/600], Loss: 2.3217\n",
      "Epoch [58/100], Step [600/600], Loss: 2.3107\n",
      "Epoch [59/100], Step [100/600], Loss: 2.3169\n",
      "Epoch [59/100], Step [200/600], Loss: 2.2994\n",
      "Epoch [59/100], Step [300/600], Loss: 2.3075\n",
      "Epoch [59/100], Step [400/600], Loss: 2.3117\n",
      "Epoch [59/100], Step [500/600], Loss: 2.3058\n",
      "Epoch [59/100], Step [600/600], Loss: 2.3241\n",
      "Epoch [60/100], Step [100/600], Loss: 2.3108\n",
      "Epoch [60/100], Step [200/600], Loss: 2.3222\n",
      "Epoch [60/100], Step [300/600], Loss: 2.3026\n",
      "Epoch [60/100], Step [400/600], Loss: 2.3224\n",
      "Epoch [60/100], Step [500/600], Loss: 2.3286\n",
      "Epoch [60/100], Step [600/600], Loss: 2.3205\n",
      "Epoch [61/100], Step [100/600], Loss: 2.3307\n",
      "Epoch [61/100], Step [200/600], Loss: 2.2928\n",
      "Epoch [61/100], Step [300/600], Loss: 2.3246\n",
      "Epoch [61/100], Step [400/600], Loss: 2.3304\n",
      "Epoch [61/100], Step [500/600], Loss: 2.3031\n",
      "Epoch [61/100], Step [600/600], Loss: 2.3066\n",
      "Epoch [62/100], Step [100/600], Loss: 2.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Step [200/600], Loss: 2.2953\n",
      "Epoch [62/100], Step [300/600], Loss: 2.2980\n",
      "Epoch [62/100], Step [400/600], Loss: 2.3044\n",
      "Epoch [62/100], Step [500/600], Loss: 2.2979\n",
      "Epoch [62/100], Step [600/600], Loss: 2.3050\n",
      "Epoch [63/100], Step [100/600], Loss: 2.3035\n",
      "Epoch [63/100], Step [200/600], Loss: 2.2980\n",
      "Epoch [63/100], Step [300/600], Loss: 2.3292\n",
      "Epoch [63/100], Step [400/600], Loss: 2.3255\n",
      "Epoch [63/100], Step [500/600], Loss: 2.3030\n",
      "Epoch [63/100], Step [600/600], Loss: 2.3363\n",
      "Epoch [64/100], Step [100/600], Loss: 2.3127\n",
      "Epoch [64/100], Step [200/600], Loss: 2.3036\n",
      "Epoch [64/100], Step [300/600], Loss: 2.2976\n",
      "Epoch [64/100], Step [400/600], Loss: 2.3177\n",
      "Epoch [64/100], Step [500/600], Loss: 2.3257\n",
      "Epoch [64/100], Step [600/600], Loss: 2.2998\n",
      "Epoch [65/100], Step [100/600], Loss: 2.3161\n",
      "Epoch [65/100], Step [200/600], Loss: 2.2998\n",
      "Epoch [65/100], Step [300/600], Loss: 2.2972\n",
      "Epoch [65/100], Step [400/600], Loss: 2.3055\n",
      "Epoch [65/100], Step [500/600], Loss: 2.3080\n",
      "Epoch [65/100], Step [600/600], Loss: 2.3275\n",
      "Epoch [66/100], Step [100/600], Loss: 2.3159\n",
      "Epoch [66/100], Step [200/600], Loss: 2.3148\n",
      "Epoch [66/100], Step [300/600], Loss: 2.3251\n",
      "Epoch [66/100], Step [400/600], Loss: 2.3174\n",
      "Epoch [66/100], Step [500/600], Loss: 2.3089\n",
      "Epoch [66/100], Step [600/600], Loss: 2.2991\n",
      "Epoch [67/100], Step [100/600], Loss: 2.3160\n",
      "Epoch [67/100], Step [200/600], Loss: 2.3077\n",
      "Epoch [67/100], Step [300/600], Loss: 2.3173\n",
      "Epoch [67/100], Step [400/600], Loss: 2.3218\n",
      "Epoch [67/100], Step [500/600], Loss: 2.3049\n",
      "Epoch [67/100], Step [600/600], Loss: 2.3243\n",
      "Epoch [68/100], Step [100/600], Loss: 2.3160\n",
      "Epoch [68/100], Step [200/600], Loss: 2.3067\n",
      "Epoch [68/100], Step [300/600], Loss: 2.3202\n",
      "Epoch [68/100], Step [400/600], Loss: 2.3175\n",
      "Epoch [68/100], Step [500/600], Loss: 2.3092\n",
      "Epoch [68/100], Step [600/600], Loss: 2.3137\n",
      "Epoch [69/100], Step [100/600], Loss: 2.3226\n",
      "Epoch [69/100], Step [200/600], Loss: 2.3059\n",
      "Epoch [69/100], Step [300/600], Loss: 2.3231\n",
      "Epoch [69/100], Step [400/600], Loss: 2.3091\n",
      "Epoch [69/100], Step [500/600], Loss: 2.3259\n",
      "Epoch [69/100], Step [600/600], Loss: 2.3080\n",
      "Epoch [70/100], Step [100/600], Loss: 2.3108\n",
      "Epoch [70/100], Step [200/600], Loss: 2.3071\n",
      "Epoch [70/100], Step [300/600], Loss: 2.3027\n",
      "Epoch [70/100], Step [400/600], Loss: 2.3074\n",
      "Epoch [70/100], Step [500/600], Loss: 2.3069\n",
      "Epoch [70/100], Step [600/600], Loss: 2.2988\n",
      "Epoch [71/100], Step [100/600], Loss: 2.3339\n",
      "Epoch [71/100], Step [200/600], Loss: 2.3119\n",
      "Epoch [71/100], Step [300/600], Loss: 2.2961\n",
      "Epoch [71/100], Step [400/600], Loss: 2.3024\n",
      "Epoch [71/100], Step [500/600], Loss: 2.3074\n",
      "Epoch [71/100], Step [600/600], Loss: 2.3087\n",
      "Epoch [72/100], Step [100/600], Loss: 2.3117\n",
      "Epoch [72/100], Step [200/600], Loss: 2.3132\n",
      "Epoch [72/100], Step [300/600], Loss: 2.3221\n",
      "Epoch [72/100], Step [400/600], Loss: 2.2993\n",
      "Epoch [72/100], Step [500/600], Loss: 2.3106\n",
      "Epoch [72/100], Step [600/600], Loss: 2.2951\n",
      "Epoch [73/100], Step [100/600], Loss: 2.3280\n",
      "Epoch [73/100], Step [200/600], Loss: 2.3048\n",
      "Epoch [73/100], Step [300/600], Loss: 2.3179\n",
      "Epoch [73/100], Step [400/600], Loss: 2.3211\n",
      "Epoch [73/100], Step [500/600], Loss: 2.3115\n",
      "Epoch [73/100], Step [600/600], Loss: 2.3097\n",
      "Epoch [74/100], Step [100/600], Loss: 2.3102\n",
      "Epoch [74/100], Step [200/600], Loss: 2.3108\n",
      "Epoch [74/100], Step [300/600], Loss: 2.2952\n",
      "Epoch [74/100], Step [400/600], Loss: 2.2964\n",
      "Epoch [74/100], Step [500/600], Loss: 2.3173\n",
      "Epoch [74/100], Step [600/600], Loss: 2.3204\n",
      "Epoch [75/100], Step [100/600], Loss: 2.3168\n",
      "Epoch [75/100], Step [200/600], Loss: 2.3081\n",
      "Epoch [75/100], Step [300/600], Loss: 2.3028\n",
      "Epoch [75/100], Step [400/600], Loss: 2.3055\n",
      "Epoch [75/100], Step [500/600], Loss: 2.3236\n",
      "Epoch [75/100], Step [600/600], Loss: 2.3044\n",
      "Epoch [76/100], Step [100/600], Loss: 2.3145\n",
      "Epoch [76/100], Step [200/600], Loss: 2.3144\n",
      "Epoch [76/100], Step [300/600], Loss: 2.2996\n",
      "Epoch [76/100], Step [400/600], Loss: 2.3149\n",
      "Epoch [76/100], Step [500/600], Loss: 2.3224\n",
      "Epoch [76/100], Step [600/600], Loss: 2.2875\n",
      "Epoch [77/100], Step [100/600], Loss: 2.3037\n",
      "Epoch [77/100], Step [200/600], Loss: 2.3114\n",
      "Epoch [77/100], Step [300/600], Loss: 2.3114\n",
      "Epoch [77/100], Step [400/600], Loss: 2.3081\n",
      "Epoch [77/100], Step [500/600], Loss: 2.3107\n",
      "Epoch [77/100], Step [600/600], Loss: 2.3151\n",
      "Epoch [78/100], Step [100/600], Loss: 2.3035\n",
      "Epoch [78/100], Step [200/600], Loss: 2.3066\n",
      "Epoch [78/100], Step [300/600], Loss: 2.2958\n",
      "Epoch [78/100], Step [400/600], Loss: 2.3217\n",
      "Epoch [78/100], Step [500/600], Loss: 2.3075\n",
      "Epoch [78/100], Step [600/600], Loss: 2.3081\n",
      "Epoch [79/100], Step [100/600], Loss: 2.2959\n",
      "Epoch [79/100], Step [200/600], Loss: 2.3044\n",
      "Epoch [79/100], Step [300/600], Loss: 2.3270\n",
      "Epoch [79/100], Step [400/600], Loss: 2.3183\n",
      "Epoch [79/100], Step [500/600], Loss: 2.3019\n",
      "Epoch [79/100], Step [600/600], Loss: 2.3141\n",
      "Epoch [80/100], Step [100/600], Loss: 2.3089\n",
      "Epoch [80/100], Step [200/600], Loss: 2.3238\n",
      "Epoch [80/100], Step [300/600], Loss: 2.3257\n",
      "Epoch [80/100], Step [400/600], Loss: 2.3168\n",
      "Epoch [80/100], Step [500/600], Loss: 2.3104\n",
      "Epoch [80/100], Step [600/600], Loss: 2.3145\n",
      "Epoch [81/100], Step [100/600], Loss: 2.2997\n",
      "Epoch [81/100], Step [200/600], Loss: 2.3067\n",
      "Epoch [81/100], Step [300/600], Loss: 2.3166\n",
      "Epoch [81/100], Step [400/600], Loss: 2.2952\n",
      "Epoch [81/100], Step [500/600], Loss: 2.3275\n",
      "Epoch [81/100], Step [600/600], Loss: 2.3081\n",
      "Epoch [82/100], Step [100/600], Loss: 2.3146\n",
      "Epoch [82/100], Step [200/600], Loss: 2.3327\n",
      "Epoch [82/100], Step [300/600], Loss: 2.2997\n",
      "Epoch [82/100], Step [400/600], Loss: 2.3097\n",
      "Epoch [82/100], Step [500/600], Loss: 2.2999\n",
      "Epoch [82/100], Step [600/600], Loss: 2.3035\n",
      "Epoch [83/100], Step [100/600], Loss: 2.3297\n",
      "Epoch [83/100], Step [200/600], Loss: 2.3118\n",
      "Epoch [83/100], Step [300/600], Loss: 2.3212\n",
      "Epoch [83/100], Step [400/600], Loss: 2.3115\n",
      "Epoch [83/100], Step [500/600], Loss: 2.3197\n",
      "Epoch [83/100], Step [600/600], Loss: 2.2968\n",
      "Epoch [84/100], Step [100/600], Loss: 2.3248\n",
      "Epoch [84/100], Step [200/600], Loss: 2.3341\n",
      "Epoch [84/100], Step [300/600], Loss: 2.3182\n",
      "Epoch [84/100], Step [400/600], Loss: 2.3059\n",
      "Epoch [84/100], Step [500/600], Loss: 2.3274\n",
      "Epoch [84/100], Step [600/600], Loss: 2.3285\n",
      "Epoch [85/100], Step [100/600], Loss: 2.3037\n",
      "Epoch [85/100], Step [200/600], Loss: 2.3139\n",
      "Epoch [85/100], Step [300/600], Loss: 2.3248\n",
      "Epoch [85/100], Step [400/600], Loss: 2.3047\n",
      "Epoch [85/100], Step [500/600], Loss: 2.3186\n",
      "Epoch [85/100], Step [600/600], Loss: 2.3156\n",
      "Epoch [86/100], Step [100/600], Loss: 2.3090\n",
      "Epoch [86/100], Step [200/600], Loss: 2.3131\n",
      "Epoch [86/100], Step [300/600], Loss: 2.3192\n",
      "Epoch [86/100], Step [400/600], Loss: 2.3124\n",
      "Epoch [86/100], Step [500/600], Loss: 2.3332\n",
      "Epoch [86/100], Step [600/600], Loss: 2.3207\n",
      "Epoch [87/100], Step [100/600], Loss: 2.3082\n",
      "Epoch [87/100], Step [200/600], Loss: 2.2957\n",
      "Epoch [87/100], Step [300/600], Loss: 2.3171\n",
      "Epoch [87/100], Step [400/600], Loss: 2.3094\n",
      "Epoch [87/100], Step [500/600], Loss: 2.3101\n",
      "Epoch [87/100], Step [600/600], Loss: 2.3120\n",
      "Epoch [88/100], Step [100/600], Loss: 2.3188\n",
      "Epoch [88/100], Step [200/600], Loss: 2.3316\n",
      "Epoch [88/100], Step [300/600], Loss: 2.3119\n",
      "Epoch [88/100], Step [400/600], Loss: 2.3042\n",
      "Epoch [88/100], Step [500/600], Loss: 2.3097\n",
      "Epoch [88/100], Step [600/600], Loss: 2.3144\n",
      "Epoch [89/100], Step [100/600], Loss: 2.3177\n",
      "Epoch [89/100], Step [200/600], Loss: 2.3216\n",
      "Epoch [89/100], Step [300/600], Loss: 2.3110\n",
      "Epoch [89/100], Step [400/600], Loss: 2.3306\n",
      "Epoch [89/100], Step [500/600], Loss: 2.3168\n",
      "Epoch [89/100], Step [600/600], Loss: 2.3004\n",
      "Epoch [90/100], Step [100/600], Loss: 2.3070\n",
      "Epoch [90/100], Step [200/600], Loss: 2.3177\n",
      "Epoch [90/100], Step [300/600], Loss: 2.3272\n",
      "Epoch [90/100], Step [400/600], Loss: 2.3029\n",
      "Epoch [90/100], Step [500/600], Loss: 2.3226\n",
      "Epoch [90/100], Step [600/600], Loss: 2.3086\n",
      "Epoch [91/100], Step [100/600], Loss: 2.3217\n",
      "Epoch [91/100], Step [200/600], Loss: 2.3021\n",
      "Epoch [91/100], Step [300/600], Loss: 2.3012\n",
      "Epoch [91/100], Step [400/600], Loss: 2.3036\n",
      "Epoch [91/100], Step [500/600], Loss: 2.3173\n",
      "Epoch [91/100], Step [600/600], Loss: 2.3380\n",
      "Epoch [92/100], Step [100/600], Loss: 2.3139\n",
      "Epoch [92/100], Step [200/600], Loss: 2.2962\n",
      "Epoch [92/100], Step [300/600], Loss: 2.3123\n",
      "Epoch [92/100], Step [400/600], Loss: 2.3326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [500/600], Loss: 2.2949\n",
      "Epoch [92/100], Step [600/600], Loss: 2.3267\n",
      "Epoch [93/100], Step [100/600], Loss: 2.3242\n",
      "Epoch [93/100], Step [200/600], Loss: 2.3007\n",
      "Epoch [93/100], Step [300/600], Loss: 2.2969\n",
      "Epoch [93/100], Step [400/600], Loss: 2.3118\n",
      "Epoch [93/100], Step [500/600], Loss: 2.3112\n",
      "Epoch [93/100], Step [600/600], Loss: 2.3209\n",
      "Epoch [94/100], Step [100/600], Loss: 2.3169\n",
      "Epoch [94/100], Step [200/600], Loss: 2.3403\n",
      "Epoch [94/100], Step [300/600], Loss: 2.3047\n",
      "Epoch [94/100], Step [400/600], Loss: 2.3052\n",
      "Epoch [94/100], Step [500/600], Loss: 2.3145\n",
      "Epoch [94/100], Step [600/600], Loss: 2.3340\n",
      "Epoch [95/100], Step [100/600], Loss: 2.3213\n",
      "Epoch [95/100], Step [200/600], Loss: 2.3245\n",
      "Epoch [95/100], Step [300/600], Loss: 2.3014\n",
      "Epoch [95/100], Step [400/600], Loss: 2.2963\n",
      "Epoch [95/100], Step [500/600], Loss: 2.3114\n",
      "Epoch [95/100], Step [600/600], Loss: 2.3238\n",
      "Epoch [96/100], Step [100/600], Loss: 2.3140\n",
      "Epoch [96/100], Step [200/600], Loss: 2.3425\n",
      "Epoch [96/100], Step [300/600], Loss: 2.3033\n",
      "Epoch [96/100], Step [400/600], Loss: 2.3071\n",
      "Epoch [96/100], Step [500/600], Loss: 2.3130\n",
      "Epoch [96/100], Step [600/600], Loss: 2.3050\n",
      "Epoch [97/100], Step [100/600], Loss: 2.3070\n",
      "Epoch [97/100], Step [200/600], Loss: 2.3264\n",
      "Epoch [97/100], Step [300/600], Loss: 2.3353\n",
      "Epoch [97/100], Step [400/600], Loss: 2.3237\n",
      "Epoch [97/100], Step [500/600], Loss: 2.3255\n",
      "Epoch [97/100], Step [600/600], Loss: 2.3037\n",
      "Epoch [98/100], Step [100/600], Loss: 2.3069\n",
      "Epoch [98/100], Step [200/600], Loss: 2.3167\n",
      "Epoch [98/100], Step [300/600], Loss: 2.2996\n",
      "Epoch [98/100], Step [400/600], Loss: 2.3197\n",
      "Epoch [98/100], Step [500/600], Loss: 2.3156\n",
      "Epoch [98/100], Step [600/600], Loss: 2.3009\n",
      "Epoch [99/100], Step [100/600], Loss: 2.3077\n",
      "Epoch [99/100], Step [200/600], Loss: 2.2860\n",
      "Epoch [99/100], Step [300/600], Loss: 2.3110\n",
      "Epoch [99/100], Step [400/600], Loss: 2.3199\n",
      "Epoch [99/100], Step [500/600], Loss: 2.3172\n",
      "Epoch [99/100], Step [600/600], Loss: 2.3123\n",
      "Epoch [100/100], Step [100/600], Loss: 2.3129\n",
      "Epoch [100/100], Step [200/600], Loss: 2.3085\n",
      "Epoch [100/100], Step [300/600], Loss: 2.3141\n",
      "Epoch [100/100], Step [400/600], Loss: 2.3149\n",
      "Epoch [100/100], Step [500/600], Loss: 2.3258\n",
      "Epoch [100/100], Step [600/600], Loss: 2.3164\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for index, (image, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # 从torch Tensor 类型转变为 Variable \n",
    "        image = Variable(image.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backword + Optimize\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算前向传播的值\n",
    "        \n",
    "        outputs = mlp_model(image)\n",
    "        \n",
    "        # 计算模型的损失函数\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 计算向后传播梯度\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (index + 1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, index+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test image: 10 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(image.view(-1, 28*28))\n",
    "    outputs = mlp_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the network on the 10000 test image: %d %%' %(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
